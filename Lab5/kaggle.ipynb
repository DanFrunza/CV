{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10157035,"sourceType":"datasetVersion","datasetId":6271266},{"sourceId":10158178,"sourceType":"datasetVersion","datasetId":6272146},{"sourceId":10159919,"sourceType":"datasetVersion","datasetId":6273500}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Input, Cropping2D, Flatten\nfrom keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport os\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n\n# Cuda\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    try:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n        print(\"GPU detected\")\n    except:\n        print(\"GPU not detected\")\n        pass\nelse :\n    print(\"GPU not detected\") \n\ndef unet_model(input_shape=(128, 128, 3)):\n    inputs = Input(input_shape)\n    \n    # Encoder\n    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    c1 = Dropout(0.1)(c1)\n    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n\n    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n    c2 = Dropout(0.2)(c2)\n    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n\n    # Bottleneck\n    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n    c3 = Dropout(0.3)(c3)\n    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n\n    # Decoder\n    u4 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3)\n    u4 = Concatenate()([u4, c2])\n    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(u4)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n\n    u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n    u5 = Concatenate()([u5, c1])\n    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n    c5 = Dropout(0.1)(c5)\n    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n\n    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n\n    model = Model(inputs, outputs)\n    return model\n\n# Load images\ndef load_images(images_folder, masks_folder, target_size=(128, 128)):\n    image_files = [f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f))]\n    mask_files = [f for f in os.listdir(masks_folder) if os.path.isfile(os.path.join(masks_folder, f))]\n\n    assert len(image_files) == len(mask_files), \"Mismatach number of images and masks.\"\n\n    images = []\n    masks = []\n\n    for img_file, mask_file in zip(image_files, mask_files):\n        img_path = os.path.join(images_folder, img_file)\n        mask_path = os.path.join(masks_folder, mask_file)\n\n        image = load_img(img_path, target_size=target_size)\n        mask = load_img(mask_path, target_size=target_size, color_mode=\"grayscale\")\n\n        # Normalize images\n        image_array = img_to_array(image) / 255.0  \n        mask_array = img_to_array(mask) / 255.0    \n\n        images.append(image_array)\n        masks.append(mask_array)\n\n    return np.array(images), np.array(masks)\n\ntrain_folder = '/kaggle/input/brain-tumor/train'\ntrain_images_folder = os.path.join(train_folder, 'images')\ntrain_masks_folder = os.path.join(train_folder, 'masks')\n\ntest_folder = '/kaggle/input/brain-tumor/test'\ntest_images_folder = os.path.join(test_folder, 'images')\ntest_masks_folder = os.path.join(test_folder, 'masks')\n\ntrain_images, train_masks = load_images(train_images_folder, train_masks_folder)\ntest_images, test_masks = load_images(test_images_folder, test_masks_folder)\n\n# Compile model\nmodel_path = '/kaggle/input/last-data/last_model.keras'\nif os.path.exists(model_path):\n    print(\"Loading existing model...\")\n    model = tf.keras.models.load_model(model_path)\nelse:\n    print(\"Creating new model...\")\n    model = unet_model(input_shape=(128, 128, 3))\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Train the model\ncheckpoint_path = \"/kaggle/working/best_model.keras\"\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    monitor='val_loss',  # Monitor validation loss\n    save_best_only=True,  # Save only the best model based on validation loss\n    verbose=1\n)\n\n# Train the model with the checkpoint callback\nhistory = model.fit(\n    train_images, train_masks, \n    validation_data=(test_images, test_masks),\n    batch_size=32,\n    epochs=20,\n    verbose=1,\n    callbacks=[checkpoint_callback]  # Add the checkpoint callback\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:45:46.503496Z","iopub.execute_input":"2024-12-10T14:45:46.503869Z"}},"outputs":[{"name":"stdout","text":"GPU not detected\nLoading existing model...\nEpoch 1/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9912 - loss: 0.0245 \nEpoch 1: val_loss improved from inf to 0.03493, saving model to /kaggle/working/best_model.keras\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 17s/step - accuracy: 0.9912 - loss: 0.0245 - val_accuracy: 0.9872 - val_loss: 0.0349\nEpoch 2/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9911 - loss: 0.0248 \nEpoch 2: val_loss improved from 0.03493 to 0.02426, saving model to /kaggle/working/best_model.keras\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1121s\u001b[0m 17s/step - accuracy: 0.9911 - loss: 0.0247 - val_accuracy: 0.9914 - val_loss: 0.0243\nEpoch 3/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9922 - loss: 0.0214 \nEpoch 3: val_loss did not improve from 0.02426\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1145s\u001b[0m 17s/step - accuracy: 0.9922 - loss: 0.0214 - val_accuracy: 0.9913 - val_loss: 0.0244\nEpoch 4/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9921 - loss: 0.0220 \nEpoch 4: val_loss did not improve from 0.02426\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1152s\u001b[0m 17s/step - accuracy: 0.9921 - loss: 0.0220 - val_accuracy: 0.9901 - val_loss: 0.0288\nEpoch 5/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9925 - loss: 0.0210 \nEpoch 5: val_loss improved from 0.02426 to 0.02320, saving model to /kaggle/working/best_model.keras\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1143s\u001b[0m 17s/step - accuracy: 0.9925 - loss: 0.0210 - val_accuracy: 0.9917 - val_loss: 0.0232\nEpoch 6/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9935 - loss: 0.0180 \nEpoch 6: val_loss did not improve from 0.02320\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1131s\u001b[0m 17s/step - accuracy: 0.9935 - loss: 0.0180 - val_accuracy: 0.9919 - val_loss: 0.0239\nEpoch 7/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9940 - loss: 0.0162 \nEpoch 7: val_loss improved from 0.02320 to 0.02244, saving model to /kaggle/working/best_model.keras\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1166s\u001b[0m 17s/step - accuracy: 0.9940 - loss: 0.0162 - val_accuracy: 0.9921 - val_loss: 0.0224\nEpoch 8/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9944 - loss: 0.0149 \nEpoch 8: val_loss did not improve from 0.02244\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1127s\u001b[0m 17s/step - accuracy: 0.9944 - loss: 0.0149 - val_accuracy: 0.9922 - val_loss: 0.0232\nEpoch 9/20\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9947 - loss: 0.0142 \nEpoch 9: val_loss did not improve from 0.02244\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1131s\u001b[0m 17s/step - accuracy: 0.9947 - loss: 0.0142 - val_accuracy: 0.9921 - val_loss: 0.0249\nEpoch 10/20\n\u001b[1m26/67\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10:17\u001b[0m 15s/step - accuracy: 0.9948 - loss: 0.0139","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Input, Cropping2D, Flatten\nfrom keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport os\n\ndef load_images(images_folder, masks_folder, target_size=(128, 128)):\n    image_files = [f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f))]\n    mask_files = [f for f in os.listdir(masks_folder) if os.path.isfile(os.path.join(masks_folder, f))]\n\n    assert len(image_files) == len(mask_files), \"Mismatach number of images and masks.\"\n\n    images = []\n    masks = []\n\n    for img_file, mask_file in zip(image_files, mask_files):\n        img_path = os.path.join(images_folder, img_file)\n        mask_path = os.path.join(masks_folder, mask_file)\n\n        image = load_img(img_path, target_size=target_size)\n        mask = load_img(mask_path, target_size=target_size, color_mode=\"grayscale\")\n\n        # Normalize images\n        image_array = img_to_array(image) / 255.0  \n        mask_array = img_to_array(mask) / 255.0    \n\n        images.append(image_array)\n        masks.append(mask_array)\n\n    return np.array(images), np.array(masks)\n\n\ntest_folder = '/kaggle/input/brain-tumor/test'\ntest_images_folder = os.path.join(test_folder, 'images')\ntest_masks_folder = os.path.join(test_folder, 'masks')\ntest_images, test_masks = load_images(test_images_folder, test_masks_folder)\n\n# Function to calculate metrics\ndef calculate_metrics(y_true, y_pred):\n    y_true = y_true.flatten()\n    y_pred = (y_pred.flatten() > 0.5).astype(int)\n\n    # Confusion matrix elements\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Pixel Accuracy\n    pixel_accuracy = (tp + tn) / (tp + tn + fp + fn)\n\n    # Jaccard's Index (IoU)\n    iou = tp / (tp + fn + fp)\n\n    # Dice Coefficient\n    dice = (2 * tp) / (2 * tp + fn + fp)\n\n    return pixel_accuracy, iou, dice\n\n# Load the best model\nmodel = tf.keras.models.load_model('/kaggle/input/model1/best_model (1).keras')\n\n\ntest_predictions = model.predict(test_images)\npixel_acc, mean_iou, dice_coeff = calculate_metrics(test_masks, test_predictions)\n\nprint(f\"Pixel Accuracy: {pixel_acc:.4f}\")\nprint(f\"Mean IoU (Jaccard's Index): {mean_iou:.4f}\")\nprint(f\"Mean Dice Coefficient: {dice_coeff:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:27:16.429181Z","iopub.execute_input":"2024-12-10T11:27:16.429644Z","iopub.status.idle":"2024-12-10T11:30:06.621029Z","shell.execute_reply.started":"2024-12-10T11:27:16.429580Z","shell.execute_reply":"2024-12-10T11:30:06.619930Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 4s/step\nPixel Accuracy: 0.9845\nMean IoU (Jaccard's Index): 0.1514\nMean Dice Coefficient: 0.2630\n","output_type":"stream"}],"execution_count":5}]}